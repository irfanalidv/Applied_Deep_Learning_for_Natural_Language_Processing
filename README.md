# Applied_Deep_Learning_for_Natural_Language_Processing

**Natural language processing (NLP)** is one of the most important technologies of the information age. 

Understanding complex language utterances is also a crucial part of artificial intelligence. Applications of **NLP** are everywhere because people communicate most everything in language: web search, advertisement, emails, customer service, language translation, radiology reports, etc. There are a large variety of underlying tasks and machine learning models powering NLP applications. Recently, **deep learning** approaches have obtained very high performance across many different NLP tasks. 
These models can often be trained with a single end-to-end model and do not require traditional, task-specific feature engineering. 

Here we will learn to implement, train, debug, visualize and invent our own neural network models. We will focus on deep excursion into cutting-edge research in deep learning applied to NLP. 
The final goal will be to involve training a complex recurrent neural network and applying it to a large scale NLP problem. 

On the model side we will come across **word vector representations, window-based neural networks, recurrent neural networks, long-short-term-memory models, recursive neural networks, convolutional neural networks** as well as some very novel models involving a memory component. 
Through hands-on approach we will learn the necessary engineering tricks for making neural networks work on practical problems.

